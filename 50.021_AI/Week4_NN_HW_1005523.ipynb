{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XCqzzlc3_Zw"
      },
      "source": [
        "# Homework Exercises\n",
        "**Due: 23th Feb, 11:59pm**\n",
        "\n",
        "**Name: Sharryl Seto (1005523)**\n",
        "\n",
        "**Class: Cl03**\n",
        "<br>\n",
        "<br>\n",
        "Based on the same FashionMNIST dataset, work on the following tasks below. Submit your homework as either: (i) an ipynb file with your results inside; or (ii) a python file and separate pdf discussing your results.\n",
        "\n",
        "(a) Develop a new feed-forward neural network that contains 3 hidden layers, with hidden layers 1, 2, 3 being of dimensions 512, 256, 128, respectively. Hidden layer 1 is the layer immediately after the input layer, while hidden layer 3 is the one just before the output layer.\n",
        "\n",
        "(b) Experiment with three different activation functions and two different optimizers. Report your results and discuss your findings.\n",
        "\n",
        "(c) Building upon Task b above, describe and implement two approaches to improve upon the best variation from Task b. Report your results and discuss your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgs23T03xBK7"
      },
      "source": [
        "# Setting up the notebook on colab\n",
        "\n",
        "Let's check if we are using the GPU environment and cuda is installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kig3C9d9D-kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966352bb-940f-4354-fa61-b2c5b27f074e"
      },
      "source": [
        "# Import PyTorch and other libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"PyTorch version:\")\n",
        "print(torch.__version__)\n",
        "print(\"GPU Detected:\")\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "#defining a shortcut function for later:\n",
        "import os\n",
        "using_GPU = os.path.exists('/opt/bin/nvidia-smi')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version:\n",
            "2.2.0+cu121\n",
            "GPU Detected:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jzL24p4YHeT"
      },
      "source": [
        "# torch nn module\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSWQMLFIPqNt"
      },
      "source": [
        "# optimisers\n",
        "import torch.optim as optim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vw-6V7XhHs3"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "We'll start by loading the data with `torchvision` --- knowing how to use torchvision isn't the point of this tutorial, so it's relatively unannotated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgbJYcQRiG2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42df7aff-1bf8-44b9-b88e-aeab133e2092"
      },
      "source": [
        "!pip install torchvision==0.17 #note: you can find compatible torch/torchvision versions here: https://github.com/pytorch/vision#installation\n",
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "train_dataset = FashionMNIST(root='./torchvision-data',\n",
        "                             train=True,\n",
        "                             transform=torchvision.transforms.ToTensor(),\n",
        "                             download=True)\n",
        "\n",
        "test_dataset = FashionMNIST(root='./torchvision-data', train=False,\n",
        "                            transform=torchvision.transforms.ToTensor())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision==0.17 in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision==0.17) (12.3.101)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchvision==0.17) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchvision==0.17) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 109958006.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 12464245.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 67238624.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 2212550.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry0GX49nj2T5"
      },
      "source": [
        "`train_dataset` and `test_dataset` are both subclasses of PyTorch's `torch.utils.data.Dataset`. The main benefit of subclassing this abstract class is that we can use `torch.utils.data.DataLoader`s to handle batching our examples and iterating over them. We'll create `DataLoader`s for our datasets now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diAuxKlRn8pZ"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data-related hyperparameters\n",
        "batch_size = 64\n",
        "\n",
        "# Set up a DataLoader for the training dataset.\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up a DataLoader for the test dataset.\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=batch_size)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWx3AsQoixs"
      },
      "source": [
        "Let's take a look at what's inside our datasets. `torch.utils.data.Dataset`s are indexable, so we can easily peek inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2eyMtSovq9"
      },
      "source": [
        "# Print the first training example\n",
        "# print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WHfersco6pG"
      },
      "source": [
        "From this output, we can see the dataset elements are tuple of `(data_tensor, label)`. `data_tensor` is a `FloatTensor` of shape `(1, 28, 28)` (since the image is 28x28), and `label` is an integer from 0 to 9 (since there are 10 classes in the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlass4LfpKyL"
      },
      "source": [
        "Let's similarly look at what the `DataLoader` produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYxZWUHFpQCD"
      },
      "source": [
        "#list(train_dataloader)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9suOk1ppR7"
      },
      "source": [
        "As we can see, the `DataLoader` groups examples into batches of size `batch_size` (64 by default in the code above). Thus, the shape of the returned tensor is `(64, 1, 28, 28)`, since we essentially stacked `batch_size` examples together. Similarly, `labels` is now a `LongTensor` of size `batch_size`.\n",
        "\n",
        "Note that the label for a single example was a Python `int` --- the dataloader automatically grouped them into a `LongTensor` of the appropriate size."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (a)\n",
        "(a) Develop a new feed-forward neural network that contains 3 hidden layers, with hidden layers 1, 2, 3 being of dimensions 512, 256, 128, respectively. Hidden layer 1 is the layer immediately after the input layer, while hidden layer 3 is the one just before the output layer."
      ],
      "metadata": {
        "id": "cQ54i20-qGwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "Now we can construct a `FeedForwardNN` instance that we'll train. Each FashionMNIST example is `28x28`, so we get it as a Tensor of shape `(28, 28)`.\n",
        "\n",
        "We'll flatten out each example to a vector of size `(784,)` for compatibility with our model."
      ],
      "metadata": {
        "id": "O38rPouv-5fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN_a(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
        "  # hidden_dim: The size of each of the hidden layers.\n",
        "  # dropout: The proportion of units to drop out after each layer.\n",
        "  def __init__(self, input_size, num_classes, num_hidden, dropout):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(FeedForwardNN_a, self).__init__()\n",
        "\n",
        "    # Set up the hidden layers.\n",
        "    assert num_hidden > 0\n",
        "    # A special ModuleList to store our hidden layers.\n",
        "    self.hidden_layers = nn.ModuleList([])\n",
        "    # First hidden layer maps from input_size -> num_hidden.\n",
        "    self.hidden_layers.append(nn.Linear(input_size, 512))\n",
        "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
        "    # Note that they can map to any dimensionality --- as long as the final\n",
        "    # output is a distribution over your classes!\n",
        "    # for i in range(num_hidden - 1):\n",
        "    self.hidden_layers.append(nn.Linear(512, 256))\n",
        "    self.hidden_layers.append(nn.Linear(256, 128))\n",
        "\n",
        "    # Set up the dropout layer.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Set up the final transform to a distribution over classes.\n",
        "    self.output_projection = nn.Linear(128, num_classes)\n",
        "\n",
        "    # Set up the nonlinearity to use between layers.\n",
        "    self.nonlinearity = nn.ReLU()\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the hidden layers, nonlinearity, and dropout.\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = hidden_layer(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.nonlinearity(x)\n",
        "\n",
        "    # Output layer: project x to a distribution over classes.\n",
        "    out = self.output_projection(x)\n",
        "\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    out_distribution = F.log_softmax(out, dim=-1)\n",
        "    return out_distribution"
      ],
      "metadata": {
        "id": "C4rIkuRXnr0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_clf_a = FeedForwardNN_a(input_size=784, num_classes=10, num_hidden=3,\n",
        "                         dropout=0.2)\n",
        "print(ffnn_clf_a)\n",
        "\n",
        "parameters = ffnn_clf_a.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])"
      ],
      "metadata": {
        "id": "MzIzeoRnjrLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67922dc1-4e03-45b5-e7bb-aa8a68b7375d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_a(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufdJjQllZ9r"
      },
      "source": [
        "If we're using a GPU, we'll move the model to the GPU which should speed up training. We do this with the same `.cuda()` method we used for Tensors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_a = ffnn_clf_a.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_a.parameters()).is_cuda)"
      ],
      "metadata": {
        "id": "ZQS_zT_Y0pxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a34ce3-9ee6-410e-a1ab-198479de65da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-zvR2a7mNBf"
      },
      "source": [
        "## Construct other classes we need for training: loss and optimizer\n",
        "\n",
        "Now, we'll set up a criterion for calculating the loss and an Optimizer for updating our parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.SGD(ffnn_clf_a.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "qAcal_BP0LXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLpxMEcPnBSS"
      },
      "source": [
        "## Train function!\n",
        "\n",
        "Now, we'll implement the procedure to train the model --- this is typically called the \"train loop\" since we loop over our batches, performing the forward pass, calculating a loss, backpropping, and then updating our parameters. This is the bulk of the code necessary to train the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train function\n",
        "def train_model(num_epochs, num_iter, fashionmnist_ffnn_clf):\n",
        "    # Iterate `num_epochs` times.\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch + 1))\n",
        "    # Iterate over the train_dataloader, unpacking the images and labels\n",
        "    for (images, labels) in train_dataloader:\n",
        "      # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784), since\n",
        "      # that's what our model expects. Remember that -1 does shape inference!\n",
        "      reshaped_images = images.view(-1, 784)\n",
        "\n",
        "      # Wrap reshaped_images and labels in Variables,\n",
        "      # since we want to calculate gradients and backprop.\n",
        "      reshaped_images = Variable(reshaped_images)\n",
        "      labels = Variable(labels)\n",
        "\n",
        "      # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
        "      if using_GPU:\n",
        "        reshaped_images = reshaped_images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "      # Run the forward pass through the model to get predicted log distribution.\n",
        "      # predicted shape: (batch_size, 10) (since there are 10 classes)\n",
        "      predicted = fashionmnist_ffnn_clf(reshaped_images)\n",
        "\n",
        "      # Calculate the loss\n",
        "      batch_loss = nll_criterion(predicted, labels)\n",
        "\n",
        "      # Clear the gradients as we prepare to backprop.\n",
        "      ffnn_optimizer.zero_grad()\n",
        "\n",
        "      # Backprop (backward pass), which calculates gradients.\n",
        "      batch_loss.backward()\n",
        "\n",
        "      # Take a gradient step to update parameters.\n",
        "      ffnn_optimizer.step()\n",
        "\n",
        "      # Increment gradient update counter.\n",
        "      num_iter += 1\n",
        "\n",
        "      # Calculate test set loss and accuracy every 500 gradient updates\n",
        "      # It's standard to have this as a separate evaluate function, but\n",
        "      # we'll place it inline for didactic purposes.\n",
        "      if num_iter % 500 == 0:\n",
        "        # Set model to eval mode, which turns off dropout.\n",
        "        fashionmnist_ffnn_clf.eval()\n",
        "        # Counters for the num of examples we get right / total num of examples.\n",
        "        num_correct = 0\n",
        "        total_examples = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # Iterate over the test dataloader\n",
        "        for (test_images, test_labels) in test_dataloader:\n",
        "          # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784) again\n",
        "          reshaped_test_images = test_images.view(-1, 784)\n",
        "\n",
        "          # Wrap test data in Variable, like we did earlier.\n",
        "          # We set volatile=True bc we don't need history; speeds up inference.\n",
        "          reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
        "          test_labels = Variable(test_labels, volatile=True)\n",
        "\n",
        "          # If we're using the GPU, move tensors to the GPU.\n",
        "          if using_GPU:\n",
        "            reshaped_test_images = reshaped_test_images.cuda()\n",
        "            test_labels = test_labels.cuda()\n",
        "\n",
        "          # Run the forward pass to get predicted distribution.\n",
        "          predicted = fashionmnist_ffnn_clf(reshaped_test_images)\n",
        "\n",
        "          # Calculate loss for this test batch. This is averaged, so multiply\n",
        "          # by the number of examples in batch to get a total.\n",
        "          total_test_loss += nll_criterion(\n",
        "              predicted, test_labels).data * test_labels.size(0)\n",
        "\n",
        "          # Get predicted labels (argmax)\n",
        "          # We need predicted.data since predicted is a Variable, and torch.max\n",
        "          # expects a Tensor as input. .data extracts Tensor underlying Variable.\n",
        "          _, predicted_labels = torch.max(predicted.data, 1)\n",
        "\n",
        "          # Count the number of examples in this batch\n",
        "          total_examples += test_labels.size(0)\n",
        "\n",
        "          # Count the total number of correctly predicted labels.\n",
        "          # predicted == labels generates a ByteTensor in indices where\n",
        "          # predicted and labels match, so we can sum to get the num correct.\n",
        "          num_correct += torch.sum(predicted_labels == test_labels.data)\n",
        "        accuracy = 100 * num_correct / total_examples\n",
        "        average_test_loss = total_test_loss / total_examples\n",
        "        print(\"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
        "            num_iter, average_test_loss, accuracy))\n",
        "        # Set the model back to train mode, which activates dropout again.\n",
        "        fashionmnist_ffnn_clf.train()"
      ],
      "metadata": {
        "id": "mhOHgGo3wFmc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(10,0,ffnn_clf_a)"
      ],
      "metadata": {
        "id": "mtkLdh0_0blk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51afc0b-37e7-4427-a156-e0abfcfbcdeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-162-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.6076332926750183. Test Accuracy 76.5199966430664.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.48759403824806213. Test Accuracy 82.18999481201172.\n",
            "Iteration 1500. Test Loss 0.45207878947257996. Test Accuracy 83.3499984741211.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.43110692501068115. Test Accuracy 84.18000030517578.\n",
            "Iteration 2500. Test Loss 0.4130193889141083. Test Accuracy 85.05999755859375.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.40974515676498413. Test Accuracy 85.47999572753906.\n",
            "Iteration 3500. Test Loss 0.39064595103263855. Test Accuracy 85.55999755859375.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.408939391374588. Test Accuracy 85.31999969482422.\n",
            "Iteration 4500. Test Loss 0.3904130160808563. Test Accuracy 85.25.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.37169697880744934. Test Accuracy 86.54000091552734.\n",
            "Iteration 5500. Test Loss 0.3568299114704132. Test Accuracy 86.66999816894531.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.3545238673686981. Test Accuracy 87.29999542236328.\n",
            "Iteration 6500. Test Loss 0.34625542163848877. Test Accuracy 87.54000091552734.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.379798024892807. Test Accuracy 86.15999603271484.\n",
            "Iteration 7500. Test Loss 0.3557535409927368. Test Accuracy 87.13999938964844.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.3446386158466339. Test Accuracy 87.54000091552734.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.3643335700035095. Test Accuracy 86.55999755859375.\n",
            "Iteration 9000. Test Loss 0.3356667757034302. Test Accuracy 87.90999603271484.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (b)\n",
        "\n",
        "(b) Experiment with three different activation functions and two different optimizers. Report your results and discuss your findings."
      ],
      "metadata": {
        "id": "pBDJMzToqZ-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 activation functions: nn.ReLU(), nn.Tanh(), nn.Sigmoid()\n",
        "\n",
        "2 optimizers: Adam and RMSProp."
      ],
      "metadata": {
        "id": "PzqnoHA2rvK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN_b(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
        "  # hidden_dim: The size of each of the hidden layers.\n",
        "  # dropout: The proportion of units to drop out after each layer.\n",
        "  def __init__(self, input_size, num_classes, num_hidden, dropout, act_func):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(FeedForwardNN_b, self).__init__()\n",
        "\n",
        "    # Set up the hidden layers.\n",
        "    assert num_hidden > 0\n",
        "    # A special ModuleList to store our hidden layers.\n",
        "    self.hidden_layers = nn.ModuleList([])\n",
        "    # First hidden layer maps from input_size -> num_hidden.\n",
        "    self.hidden_layers.append(nn.Linear(input_size, 512))\n",
        "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
        "    # Note that they can map to any dimensionality --- as long as the final\n",
        "    # output is a distribution over your classes!\n",
        "    # for i in range(num_hidden - 1):\n",
        "    self.hidden_layers.append(nn.Linear(512, 256))\n",
        "    self.hidden_layers.append(nn.Linear(256, 128))\n",
        "\n",
        "    # Set up the dropout layer.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Set up the final transform to a distribution over classes.\n",
        "    self.output_projection = nn.Linear(128, num_classes)\n",
        "\n",
        "    # Set up the nonlinearity to use between layers.\n",
        "    if act_func== \"ReLU\":\n",
        "      self.nonlinearity = nn.ReLU()\n",
        "    elif act_func==\"Tanh\":\n",
        "      self.nonlinearity = nn.Tanh()\n",
        "    elif act_func==\"Sigmoid\":\n",
        "      self.nonlinearity = nn.Sigmoid()\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the hidden layers, nonlinearity, and dropout.\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = hidden_layer(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.nonlinearity(x)\n",
        "\n",
        "    # Output layer: project x to a distribution over classes.\n",
        "    out = self.output_projection(x)\n",
        "\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    out_distribution = F.log_softmax(out, dim=-1)\n",
        "    return out_distribution"
      ],
      "metadata": {
        "id": "alnqlDEKqY2I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 1A: ReLU + Adam"
      ],
      "metadata": {
        "id": "k_enV1RbvV_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variation 1: ReLU\n",
        "ffnn_clf_b1 = FeedForwardNN_b(input_size=784, num_classes=10, num_hidden=3,\n",
        "                         dropout=0.2, act_func=\"ReLU\")\n",
        "print(ffnn_clf_b1)"
      ],
      "metadata": {
        "id": "qpurJxEItWeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778cddaa-303f-4a7e-df56-3e65b1fb6e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_b(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_b1 = ffnn_clf_b1.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_b1.parameters()).is_cuda)"
      ],
      "metadata": {
        "id": "6KruzKzaxS0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0976e239-9ba9-415f-b0de-d0dec27712d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.Adam(ffnn_clf_b1.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "vNYfOc3GvUtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b1)"
      ],
      "metadata": {
        "id": "RdkS9YjHw8uR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce96653-c7fa-4178-89f6-1d4e1fdadf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-162-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.5165243744850159. Test Accuracy 82.47000122070312.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.47114139795303345. Test Accuracy 83.54000091552734.\n",
            "Iteration 1500. Test Loss 0.4532720148563385. Test Accuracy 84.1500015258789.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.44893041253089905. Test Accuracy 84.38999938964844.\n",
            "Iteration 2500. Test Loss 0.45002636313438416. Test Accuracy 84.06999969482422.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.4376470148563385. Test Accuracy 84.57999420166016.\n",
            "Iteration 3500. Test Loss 0.4363155663013458. Test Accuracy 84.65999603271484.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.4360687732696533. Test Accuracy 85.05999755859375.\n",
            "Iteration 4500. Test Loss 0.4236913323402405. Test Accuracy 84.97999572753906.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.43823176622390747. Test Accuracy 84.75.\n",
            "Iteration 5500. Test Loss 0.42339402437210083. Test Accuracy 85.30999755859375.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.41598114371299744. Test Accuracy 85.04999542236328.\n",
            "Iteration 6500. Test Loss 0.4119493067264557. Test Accuracy 85.38999938964844.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.4166482388973236. Test Accuracy 84.98999786376953.\n",
            "Iteration 7500. Test Loss 0.47691795229911804. Test Accuracy 82.55999755859375.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.4110763669013977. Test Accuracy 85.73999786376953.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4587395489215851. Test Accuracy 84.54999542236328.\n",
            "Iteration 9000. Test Loss 0.4513688385486603. Test Accuracy 85.11000061035156.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 1B: ReLU + RMSProp"
      ],
      "metadata": {
        "id": "0iY4SqGCzOZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_b1.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "MqO0x1cQzTUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b1)"
      ],
      "metadata": {
        "id": "jofKiCciz0BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f632dcc1-4539-4c59-9cca-240812715ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-162-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.482811838388443. Test Accuracy 85.0.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.45666414499282837. Test Accuracy 84.97999572753906.\n",
            "Iteration 1500. Test Loss 0.42218005657196045. Test Accuracy 85.70999908447266.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.43939346075057983. Test Accuracy 85.7699966430664.\n",
            "Iteration 2500. Test Loss 0.4716459810733795. Test Accuracy 83.95999908447266.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.4900510013103485. Test Accuracy 85.0.\n",
            "Iteration 3500. Test Loss 0.46308377385139465. Test Accuracy 84.72999572753906.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.4534662067890167. Test Accuracy 85.11000061035156.\n",
            "Iteration 4500. Test Loss 0.44308093190193176. Test Accuracy 85.36000061035156.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.49480628967285156. Test Accuracy 84.06999969482422.\n",
            "Iteration 5500. Test Loss 0.4244786500930786. Test Accuracy 86.12999725341797.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.42973342537879944. Test Accuracy 86.36000061035156.\n",
            "Iteration 6500. Test Loss 0.5059805512428284. Test Accuracy 85.75.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.44201311469078064. Test Accuracy 85.9000015258789.\n",
            "Iteration 7500. Test Loss 0.4221920669078827. Test Accuracy 85.95999908447266.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.4691995084285736. Test Accuracy 84.90999603271484.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4353955388069153. Test Accuracy 85.97999572753906.\n",
            "Iteration 9000. Test Loss 0.4654547870159149. Test Accuracy 85.0199966430664.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 2A: Tanh + Adam\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kd_apckfZaI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variation 2: Tanh\n",
        "ffnn_clf_b2 = FeedForwardNN_b(input_size=784, num_classes=10, num_hidden=3,\n",
        "                         dropout=0.2, act_func=\"Tanh\")\n",
        "print(ffnn_clf_b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U62f9gBZZt-j",
        "outputId": "999d7ef7-bbaa-4143-b7d0-9c6ccc8a093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_b(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Tanh()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_b2 = ffnn_clf_b2.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_b2.parameters()).is_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5YE_mW7Z3tB",
        "outputId": "7a27cbe7-8448-49f4-c328-2697deb8057e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.Adam(ffnn_clf_b2.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "2-f230QaZ652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ebRoAtuZ9pu",
        "outputId": "49021db7-9b13-4f56-f352-50b2b08657ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-16-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.6670293807983398. Test Accuracy 77.58999633789062.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5968989133834839. Test Accuracy 78.16999816894531.\n",
            "Iteration 1500. Test Loss 0.5965826511383057. Test Accuracy 80.41999816894531.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.6133263111114502. Test Accuracy 80.02999877929688.\n",
            "Iteration 2500. Test Loss 0.5700254440307617. Test Accuracy 80.04000091552734.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.5751953721046448. Test Accuracy 80.79000091552734.\n",
            "Iteration 3500. Test Loss 0.5977470874786377. Test Accuracy 80.0199966430664.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.6000685691833496. Test Accuracy 77.54999542236328.\n",
            "Iteration 4500. Test Loss 0.5947855114936829. Test Accuracy 80.68000030517578.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.6131851077079773. Test Accuracy 79.45999908447266.\n",
            "Iteration 5500. Test Loss 0.5812848210334778. Test Accuracy 78.29999542236328.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5622751116752625. Test Accuracy 80.83999633789062.\n",
            "Iteration 6500. Test Loss 0.575005829334259. Test Accuracy 79.63999938964844.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.598563015460968. Test Accuracy 80.45999908447266.\n",
            "Iteration 7500. Test Loss 0.5805901288986206. Test Accuracy 80.50999450683594.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.5549321174621582. Test Accuracy 79.57999420166016.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.6516752243041992. Test Accuracy 79.44999694824219.\n",
            "Iteration 9000. Test Loss 0.613991916179657. Test Accuracy 79.82999420166016.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r3vRBn4YoC5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 2B: Tanh + RMSprop\n"
      ],
      "metadata": {
        "id": "FYLACYuga5Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_b2.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "dYkOH9jma5BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q-XZMzVa_3n",
        "outputId": "869d5bb0-ceb1-4e35-8775-bd83aff5cbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-16-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.5585297346115112. Test Accuracy 81.04000091552734.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5351490378379822. Test Accuracy 82.43999481201172.\n",
            "Iteration 1500. Test Loss 0.5927719473838806. Test Accuracy 80.3699951171875.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.5723657011985779. Test Accuracy 81.3499984741211.\n",
            "Iteration 2500. Test Loss 0.5930774807929993. Test Accuracy 80.0.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.553825855255127. Test Accuracy 81.40999603271484.\n",
            "Iteration 3500. Test Loss 0.5374708771705627. Test Accuracy 81.54000091552734.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.5385926961898804. Test Accuracy 82.3699951171875.\n",
            "Iteration 4500. Test Loss 0.5849688649177551. Test Accuracy 80.61000061035156.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.5565933585166931. Test Accuracy 81.47999572753906.\n",
            "Iteration 5500. Test Loss 0.5417063236236572. Test Accuracy 81.50999450683594.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5608379244804382. Test Accuracy 79.79999542236328.\n",
            "Iteration 6500. Test Loss 0.5738974809646606. Test Accuracy 81.47000122070312.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.5559714436531067. Test Accuracy 81.22999572753906.\n",
            "Iteration 7500. Test Loss 0.5645595788955688. Test Accuracy 81.44999694824219.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.5573295950889587. Test Accuracy 82.12999725341797.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.5542755126953125. Test Accuracy 81.94999694824219.\n",
            "Iteration 9000. Test Loss 0.5963281393051147. Test Accuracy 80.31999969482422.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 3A: Sigmoid + Adam"
      ],
      "metadata": {
        "id": "z7D7TE2mbbnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variation 3: Sigmoid\n",
        "ffnn_clf_b3 = FeedForwardNN_b(input_size=784, num_classes=10, num_hidden=3,\n",
        "                         dropout=0.2, act_func=\"Sigmoid\")\n",
        "print(ffnn_clf_b3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvYcjAISbh0t",
        "outputId": "26af4b7c-87dc-4437-839a-da9c4aaee973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_b(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_b3 = ffnn_clf_b3.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_b3.parameters()).is_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgKRJVjxbl-W",
        "outputId": "155524f8-5e9b-44fa-b60b-fdcee6cb7d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.Adam(ffnn_clf_b3.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "ATzhabhKbpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtxqUawtbuus",
        "outputId": "0fba1a94-67ea-4c3e-9d2c-72c658a3daa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-16-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.63265460729599. Test Accuracy 77.30999755859375.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5842671990394592. Test Accuracy 80.88999938964844.\n",
            "Iteration 1500. Test Loss 0.596457302570343. Test Accuracy 81.7699966430664.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.5717397332191467. Test Accuracy 82.32999420166016.\n",
            "Iteration 2500. Test Loss 0.5036637783050537. Test Accuracy 83.88999938964844.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.47630736231803894. Test Accuracy 84.56999969482422.\n",
            "Iteration 3500. Test Loss 0.5322922468185425. Test Accuracy 84.12999725341797.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.5059822201728821. Test Accuracy 84.33999633789062.\n",
            "Iteration 4500. Test Loss 0.49571073055267334. Test Accuracy 85.00999450683594.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.5630818009376526. Test Accuracy 83.86000061035156.\n",
            "Iteration 5500. Test Loss 0.49617528915405273. Test Accuracy 85.04000091552734.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5156679749488831. Test Accuracy 84.75999450683594.\n",
            "Iteration 6500. Test Loss 0.4868362545967102. Test Accuracy 85.32999420166016.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.478078156709671. Test Accuracy 85.58999633789062.\n",
            "Iteration 7500. Test Loss 0.45577704906463623. Test Accuracy 85.25.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.4603918194770813. Test Accuracy 85.3699951171875.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4362490475177765. Test Accuracy 86.37999725341797.\n",
            "Iteration 9000. Test Loss 0.4699700176715851. Test Accuracy 86.04999542236328.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variation 3B: Sigmoid + RMSprop"
      ],
      "metadata": {
        "id": "81_HMlZxb4MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_b3.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "04vU8gSfb39c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_b3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLXQVHVhb37v",
        "outputId": "dbfaef8a-4cd3-462c-9e4b-15225e885912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-16-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.43482404947280884. Test Accuracy 86.18000030517578.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.45174023509025574. Test Accuracy 85.93999481201172.\n",
            "Iteration 1500. Test Loss 0.4200459420681. Test Accuracy 86.41999816894531.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.43779149651527405. Test Accuracy 86.37999725341797.\n",
            "Iteration 2500. Test Loss 0.4491100013256073. Test Accuracy 85.72000122070312.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.43674716353416443. Test Accuracy 86.25999450683594.\n",
            "Iteration 3500. Test Loss 0.4236128330230713. Test Accuracy 86.55999755859375.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.43589621782302856. Test Accuracy 86.22999572753906.\n",
            "Iteration 4500. Test Loss 0.4324311316013336. Test Accuracy 86.3499984741211.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.4161583483219147. Test Accuracy 86.6500015258789.\n",
            "Iteration 5500. Test Loss 0.45435765385627747. Test Accuracy 86.11000061035156.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.4172724485397339. Test Accuracy 86.83999633789062.\n",
            "Iteration 6500. Test Loss 0.4357503354549408. Test Accuracy 86.43000030517578.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.42935994267463684. Test Accuracy 86.2699966430664.\n",
            "Iteration 7500. Test Loss 0.4382316768169403. Test Accuracy 85.98999786376953.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.4662754237651825. Test Accuracy 85.7699966430664.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.42501190304756165. Test Accuracy 86.7699966430664.\n",
            "Iteration 9000. Test Loss 0.4451184570789337. Test Accuracy 86.1199951171875.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "lr = 0.005\n",
        "\n",
        "Variation 1: ReLU + Adam / RMSprop\n",
        "- Test Loss 0.4513688385486603 / 0.4654547870159149.\n",
        "-Test Accuracy 85.11000061035156 / 85.0199966430664.\n",
        "\n",
        "Variation 2: Tanh + Adam / RMSprop\n",
        "- Test Loss 0.613991916179657 / 0.5963281393051147.\n",
        "- Test Accuracy 79.82999420166016 / 80.31999969482422.\n",
        "\n",
        "Variation 3: Sigmoid + Adam / RMSprop\n",
        "- Test Loss 0.4699700176715851 / 0.4451184570789337.\n",
        "- Test Accuracy 86.04999542236328 / 86.1199951171875."
      ],
      "metadata": {
        "id": "kqf7QcWZYjMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part (c)\n",
        "\n",
        "(c) Building upon Task b above, describe and implement two approaches to improve upon the best variation from Task b. Report your results and discuss your findings."
      ],
      "metadata": {
        "id": "4J1TPyTfqbbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Best variation from Task B: **Sigmoid + RMSprop**\n"
      ],
      "metadata": {
        "id": "aAd9M1x6Yaah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1: Increase number of epochs and number of hidden layers (to a limit)\n"
      ],
      "metadata": {
        "id": "aZHyxm-4mApI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_clf_c = FeedForwardNN_b(input_size=784, num_classes=10, num_hidden=5,\n",
        "                         dropout=0.2, act_func=\"Sigmoid\")\n",
        "print(ffnn_clf_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NSpkUrZn5vI",
        "outputId": "b806ae94-981e-41a9-be32-70c8f4d792ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_b(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_c = ffnn_clf_c.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_c.parameters()).is_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgFma5a2o5Pu",
        "outputId": "87136020-b447-4c4f-ab33-7511774f0bd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_c.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "ujsdqv8EpJkw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train  with 15 epochs\n",
        "train_model(15, 0, ffnn_clf_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQezgDHPpM87",
        "outputId": "0e00f1d1-8dc5-4d37-87c4-7795f134cd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-162-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 1.080093502998352. Test Accuracy 58.099998474121094.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.6740158200263977. Test Accuracy 77.0199966430664.\n",
            "Iteration 1500. Test Loss 0.5758374333381653. Test Accuracy 80.73999786376953.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.6483395099639893. Test Accuracy 79.30999755859375.\n",
            "Iteration 2500. Test Loss 0.5413018465042114. Test Accuracy 83.13999938964844.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.5594813823699951. Test Accuracy 82.98999786376953.\n",
            "Iteration 3500. Test Loss 0.4925706386566162. Test Accuracy 84.50999450683594.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.4928855299949646. Test Accuracy 84.87999725341797.\n",
            "Iteration 4500. Test Loss 0.49439021944999695. Test Accuracy 85.00999450683594.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.4545965790748596. Test Accuracy 85.72000122070312.\n",
            "Iteration 5500. Test Loss 0.46522656083106995. Test Accuracy 84.75.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.4562705457210541. Test Accuracy 85.6199951171875.\n",
            "Iteration 6500. Test Loss 0.48503732681274414. Test Accuracy 85.18999481201172.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.45837968587875366. Test Accuracy 85.38999938964844.\n",
            "Iteration 7500. Test Loss 0.4426688849925995. Test Accuracy 85.22999572753906.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.43699881434440613. Test Accuracy 86.16999816894531.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.48674431443214417. Test Accuracy 85.18999481201172.\n",
            "Iteration 9000. Test Loss 0.4302971065044403. Test Accuracy 85.75.\n",
            "Starting epoch 11\n",
            "Iteration 9500. Test Loss 0.4242524206638336. Test Accuracy 86.15999603271484.\n",
            "Iteration 10000. Test Loss 0.42969945073127747. Test Accuracy 86.04000091552734.\n",
            "Starting epoch 12\n",
            "Iteration 10500. Test Loss 0.4343799352645874. Test Accuracy 85.37999725341797.\n",
            "Iteration 11000. Test Loss 0.4334009289741516. Test Accuracy 86.0999984741211.\n",
            "Starting epoch 13\n",
            "Iteration 11500. Test Loss 0.4459312856197357. Test Accuracy 84.93999481201172.\n",
            "Iteration 12000. Test Loss 0.46665632724761963. Test Accuracy 86.15999603271484.\n",
            "Starting epoch 14\n",
            "Iteration 12500. Test Loss 0.4289814233779907. Test Accuracy 85.93000030517578.\n",
            "Iteration 13000. Test Loss 0.42411375045776367. Test Accuracy 86.56999969482422.\n",
            "Starting epoch 15\n",
            "Iteration 13500. Test Loss 0.4327354431152344. Test Accuracy 86.08999633789062.\n",
            "Iteration 14000. Test Loss 0.4037586450576782. Test Accuracy 86.73999786376953.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding 1:\n",
        "- increasing the no. of epochs from 10 to 15 increases the accuracy slightly, but from the trend we can see that this only resulted in the oscillation of the max. accuracy of about 86.5.\n",
        "\n",
        "Result\n",
        "- Test Loss decreased by 0.04 from 0.4451184570789337 to 0.4037586450576782.\n",
        "- Test Accuracy increased by 0.6 from 86.1199951171875 to 86.73999786376953."
      ],
      "metadata": {
        "id": "ZkeCq278pfdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# changing no. of layers\n",
        "class FeedForwardNN_c(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
        "  # hidden_dim: The size of each of the hidden layers.\n",
        "  # dropout: The proportion of units to drop out after each layer.\n",
        "  def __init__(self, input_size, num_classes, num_hidden, dropout):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(FeedForwardNN_c, self).__init__()\n",
        "\n",
        "    # Set up the hidden layers.\n",
        "    assert num_hidden > 0\n",
        "    # A special ModuleList to store our hidden layers.\n",
        "    self.hidden_layers = nn.ModuleList([])\n",
        "    # First hidden layer maps from input_size -> num_hidden.\n",
        "    self.hidden_layers.append(nn.Linear(input_size, 512))\n",
        "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
        "    # Note that they can map to any dimensionality --- as long as the final\n",
        "    # output is a distribution over your classes!\n",
        "    # for i in range(num_hidden - 1):\n",
        "    self.hidden_layers.append(nn.Linear(512, 256))\n",
        "    self.hidden_layers.append(nn.Linear(256, 128))\n",
        "    self.hidden_layers.append(nn.Linear(128, 64))\n",
        "\n",
        "    # Set up the dropout layer.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Set up the final transform to a distribution over classes.\n",
        "    self.output_projection = nn.Linear(64, num_classes)\n",
        "\n",
        "    # Set up the nonlinearity to use between layers.\n",
        "    self.nonlinearity = nn.Sigmoid()\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the hidden layers, nonlinearity, and dropout.\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = hidden_layer(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.nonlinearity(x)\n",
        "\n",
        "    # Output layer: project x to a distribution over classes.\n",
        "    out = self.output_projection(x)\n",
        "\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    out_distribution = F.log_softmax(out, dim=-1)\n",
        "    return out_distribution"
      ],
      "metadata": {
        "id": "vEZWYcnkvSsM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_clf_c1 = FeedForwardNN_c(input_size=784, num_classes=10, num_hidden=4,\n",
        "                         dropout=0.2)\n",
        "print(ffnn_clf_c1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYMLIABxubdp",
        "outputId": "432e9291-6af6-45b2-966d-09923926b36d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNN_c(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (nonlinearity): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if using_GPU:\n",
        "  ffnn_clf_c1 = ffnn_clf_c1.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(ffnn_clf_c1.parameters()).is_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO30oEGyunAo",
        "outputId": "5fc902fd-28f8-4748-99bb-74df16950b75"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on GPU?:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.005\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_c1.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "FnbgBMegxq-I"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_c1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48TZ7jeugss",
        "outputId": "2cfa7c21-b1ae-45a3-de2d-72f21305eef1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-10-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.4462265372276306. Test Accuracy 85.47999572753906.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.42653006315231323. Test Accuracy 86.13999938964844.\n",
            "Iteration 1500. Test Loss 0.4461647868156433. Test Accuracy 84.88999938964844.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.44366443157196045. Test Accuracy 85.97000122070312.\n",
            "Iteration 2500. Test Loss 0.4399830996990204. Test Accuracy 85.62999725341797.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.42525726556777954. Test Accuracy 86.02999877929688.\n",
            "Iteration 3500. Test Loss 0.45552483201026917. Test Accuracy 84.65999603271484.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.41470015048980713. Test Accuracy 86.18999481201172.\n",
            "Iteration 4500. Test Loss 0.41976916790008545. Test Accuracy 85.91999816894531.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.4269491136074066. Test Accuracy 85.77999877929688.\n",
            "Iteration 5500. Test Loss 0.4645144045352936. Test Accuracy 84.72000122070312.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.42072156071662903. Test Accuracy 85.90999603271484.\n",
            "Iteration 6500. Test Loss 0.40526506304740906. Test Accuracy 86.37999725341797.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.4356522262096405. Test Accuracy 85.38999938964844.\n",
            "Iteration 7500. Test Loss 0.4293651878833771. Test Accuracy 86.07999420166016.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.4458240568637848. Test Accuracy 84.97000122070312.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4317237138748169. Test Accuracy 85.97000122070312.\n",
            "Iteration 9000. Test Loss 0.3939173221588135. Test Accuracy 86.50999450683594.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding 2:\n",
        "- despite increasing the no. of hidden layers from 3 to 4, accuracy stays limited at about 86.5 maximum."
      ],
      "metadata": {
        "id": "fE1cWI2WuT6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Method 2: Decrease learning rate and change loss functions\n"
      ],
      "metadata": {
        "id": "yTrUW0r5mAKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.001\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_c1.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "tnzbgSPBnjx4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_c1)"
      ],
      "metadata": {
        "id": "Pjr0O92Rqeab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05edc485-3d3e-403b-a9c0-a8a9dcab987e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-10-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.3976600170135498. Test Accuracy 86.62999725341797.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.39026811718940735. Test Accuracy 86.6199951171875.\n",
            "Iteration 1500. Test Loss 0.38605162501335144. Test Accuracy 86.8699951171875.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.39221036434173584. Test Accuracy 87.1199951171875.\n",
            "Iteration 2500. Test Loss 0.3846086859703064. Test Accuracy 87.15999603271484.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.375472754240036. Test Accuracy 87.6500015258789.\n",
            "Iteration 3500. Test Loss 0.37506869435310364. Test Accuracy 87.43999481201172.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.3770142197608948. Test Accuracy 87.32999420166016.\n",
            "Iteration 4500. Test Loss 0.3668029308319092. Test Accuracy 87.56999969482422.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.3716893196105957. Test Accuracy 87.4000015258789.\n",
            "Iteration 5500. Test Loss 0.3712044954299927. Test Accuracy 87.32999420166016.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.37441182136535645. Test Accuracy 87.55999755859375.\n",
            "Iteration 6500. Test Loss 0.36493101716041565. Test Accuracy 87.56999969482422.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.3660815358161926. Test Accuracy 87.47000122070312.\n",
            "Iteration 7500. Test Loss 0.3668634295463562. Test Accuracy 87.32999420166016.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.3680781424045563. Test Accuracy 87.16999816894531.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.36828863620758057. Test Accuracy 87.68999481201172.\n",
            "Iteration 9000. Test Loss 0.37224194407463074. Test Accuracy 87.56999969482422.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding 1:\n",
        "- the limiting factor was indeed the learning rate, as decreasing from 0.005 to 0.001 it allowed the accuracy to increase by about 1.\n"
      ],
      "metadata": {
        "id": "uK2bNapH0XI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 0.001\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.RMSprop(ffnn_clf_c1.parameters(),\n",
        "                           lr=lr)"
      ],
      "metadata": {
        "id": "9V1tmq0p0uVA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(10, 0, ffnn_clf_c1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9jMb-af1RAd",
        "outputId": "a951f8be-ef8a-4186-c5cd-3ed75251fda1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b6fad9d0efe0>:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-10-b6fad9d0efe0>:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500. Test Loss 0.3659091591835022. Test Accuracy 87.13999938964844.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.3606683909893036. Test Accuracy 87.29999542236328.\n",
            "Iteration 1500. Test Loss 0.36331987380981445. Test Accuracy 87.73999786376953.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.36198291182518005. Test Accuracy 87.79999542236328.\n",
            "Iteration 2500. Test Loss 0.3564203977584839. Test Accuracy 87.95999908447266.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.35864007472991943. Test Accuracy 87.7699966430664.\n",
            "Iteration 3500. Test Loss 0.3625774681568146. Test Accuracy 87.7699966430664.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.36144399642944336. Test Accuracy 87.70999908447266.\n",
            "Iteration 4500. Test Loss 0.3569597601890564. Test Accuracy 87.79000091552734.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.36297717690467834. Test Accuracy 87.65999603271484.\n",
            "Iteration 5500. Test Loss 0.361416757106781. Test Accuracy 87.8699951171875.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.3633391559123993. Test Accuracy 87.43999481201172.\n",
            "Iteration 6500. Test Loss 0.35730820894241333. Test Accuracy 87.75.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.3539808392524719. Test Accuracy 87.87999725341797.\n",
            "Iteration 7500. Test Loss 0.3556748032569885. Test Accuracy 87.75999450683594.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.357939213514328. Test Accuracy 87.5999984741211.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.3604418635368347. Test Accuracy 87.68999481201172.\n",
            "Iteration 9000. Test Loss 0.35716548562049866. Test Accuracy 88.0199966430664.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding 2:\n",
        "- changing the loss function to CrossEntropyLoss allowed the accuracy to increase by about 0.5.\n"
      ],
      "metadata": {
        "id": "MaPiURhe1yCD"
      }
    }
  ]
}